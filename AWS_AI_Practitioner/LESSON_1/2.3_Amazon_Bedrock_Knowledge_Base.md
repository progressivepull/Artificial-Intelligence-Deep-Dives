# 2.3 Amazon Bedrock Knowledge Base

Amazon Bedrock Knowledge Base is a fully managed service that allows you to securely connect a foundation model (FM) with your internal, proprietary data. It is designed to implement the entire Retrieval Augmented Generation (RAG) workflow, from data ingestion to retrieval and prompt augmentation. This capability enables generative AI applications to produce more relevant, accurate, and customized responses by providing models with up-to-date, private information, without requiring you to retrain the underlying model.

---

## How Bedrock Knowledge Base Works

The service automates the complex, multi-step process of RAG:

- **Ingestion**:  
  You provide your data from sources such as an Amazon S3 bucket, Confluence, Microsoft SharePoint, Salesforce, or a web crawler. The service then fetches this data, breaks it into smaller chunks, and converts the text into numerical vector embeddings.

- **Indexing**:  
  The vector embeddings are automatically stored in a vector database of your choice, such as Amazon OpenSearch Serverless, Amazon Aurora, or Pinecone. This database acts as a searchable index of your proprietary knowledge.

- **Retrieval**:  
  When a user submits a query, Bedrock first retrieves the most relevant and context-rich text chunks from your vector database.

- **Generation**:  
  The retrieved information is then used to augment the prompt sent to the FM. The FM generates a response based on this new context.

- **Source Attribution**:  
  The generated response is returned to the user along with citations that trace the information back to its source document, minimizing hallucinations and improving transparency.

---

## Key Features and Benefits

- **Fully Managed RAG**:  
  Automates the entire RAG pipeline, eliminating the need to build and manage custom integrations and data flows.

- **Structured Data Support**:  
  Includes built-in capabilities to convert natural language queries into SQL, allowing FMs to retrieve structured data from sources like data warehouses and lakes.

- **Customizable Retrieval**:  
  Offers advanced data chunking options (like semantic or hierarchical chunking), metadata filtering, and the use of reranker models to optimize retrieval accuracy.

- **Evaluation Tools**:  
  Includes RAG evaluation tools that allow you to systematically assess the quality of both retrieval and generation stages, enabling data-driven optimizations.

- **Serverless and Cost-Effective**:  
  The service is serverless, and you are charged only for the models, vector database, and storage you use, making it cost-effective for variable workloads.

- **Integration with Agents**:  
  Knowledge Bases can be used as a tool for Amazon Bedrock Agents, allowing them to provide contextual information for multi-step reasoning tasks.

---

## Limitations

- **Query Length**:  
  Queries to the knowledge base have a character limit of 1,000, which may require you to split longer queries into multiple parts.

- **Ingestion Concurrency**:  
  There are limits on the number of concurrent ingestion requests that can be processed at one time.

- **Regional Availability**:  
  While generally available, some features might be limited to specific AWS regions.

- **Storage Constraints**:  
  Customers have reported large collection sizes in the vector database for a relatively small amount of data, which warrants optimization for cost management.

---

## Resources

- [Demo: Amazon Bedrock Knowledge Bases - Structured Data Retrieval | Amazon Web Services](https://www.youtube.com/watch?v=2Wd6Ig0mDe4)  
- [AWS re:Invent 2024 - Build scalable RAG applications using Amazon Bedrock Knowledge Bases (AIM305)](https://www.youtube.com/watch?v=jSlNfr8Uuco)


## [Context](./../context.md)
