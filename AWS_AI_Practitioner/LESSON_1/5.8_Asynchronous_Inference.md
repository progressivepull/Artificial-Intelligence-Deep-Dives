# 5.8 Asynchronous Inference

# Asynchronous Inference in Machine Learning

## Overview

Asynchronous inference is an AI deployment pattern where prediction requests are processed from a queue rather than immediately in response to an API call. Unlike real-time inference, the client does not wait for a response but receives a notification once processing is complete.

This approach is ideal for large or complex AI models with unpredictable traffic patterns and longer processing times.

## How Asynchronous Inference Works

An asynchronous inference workflow typically involves the following steps:

- **Request Submission**  
  The client sends an inference request to an endpoint, often referencing input data stored in cloud storage (e.g., Amazon S3) for large payloads.

- **Queueing**  
  The request is placed in a managed queue. The service returns a confirmation (e.g., request ID or output location), allowing the client to proceed with other tasks.

- **Asynchronous Processing**  
  The inference endpoint pulls requests from the queue and processes them individually or in small batches. It scales automatically based on queue size and workload.

- **Notification**  
  Once processing is complete, results are stored in a designated output location. The client is notified via services like Amazon SNS or a webhook.

- **Result Retrieval**  
  The client retrieves the results using the output location provided in the initial response.

## Key Benefits and Trade-Offs

| Aspect          | Benefits                                                                 | Trade-Offs                                                                 |
|----------------|--------------------------------------------------------------------------|----------------------------------------------------------------------------|
| **Cost**        | Efficient billing: pay only for active compute time. Scales to zero when idle. | Not suitable for low-latency applications due to queuing delays.           |
| **Scalability** | Absorbs bursty traffic without request failures.                         | Cold starts may introduce delays when scaling from zero.                   |
| **Payload Size**| Supports large inputs (e.g., up to 1 GB). Ideal for documents or images. | Requires more complex architecture with cloud storage and notifications.   |
| **Processing Time** | Robust for long-running jobs. Avoids connection timeouts.               | Delayed response requires downstream systems to handle latency.            |

## Common Use Cases

- **Large Language Model (LLM) Processing**  
  Summarizing or embedding large documents where immediate response is not critical.

- **Computer Vision**  
  Analyzing high-resolution images or long videos for object detection.

- **Speech and Audio Processing**  
  Transcribing lengthy audio recordings or analyzing video streams.

- **Research and Analytics**  
  Running large batches of inference tasks for data backfilling or scheduled reporting.


* [Amazon SageMaker Asynchronous Inference Explained + Tutorial](https://www.youtube.com/watch?v=MVPlUtYIB6Q)
* [Hybrid Hosting with SageMaker AI Asynchronous Inference](https://www.youtube.com/watch?v=7ZMlI9ZNtNI)
* [AWS On Air ft. Amazon SageMaker Asynchronous Inference | AWS Events](https://www.youtube.com/watch?v=yhYCzByjzg8)

## [Context](./../context.md)




