# 📊 R-squared Score

The **R-squared (R²) score**, also known as the **coefficient of determination**, is a statistical measure that indicates how well the independent variable(s) in a regression model explain the variation in the dependent variable. It represents the proportion of the dependent variable's variance that is predictable from the independent variable(s).

---

## 🎯 Interpretation of R-squared

R-squared values range from 0 to 1:

- **R² = 0**:  
  The model explains none of the variability. It's no better than using the mean of the dependent variable.

- **R² = 1**:  
  The model explains all variability. Predictions perfectly match the observed data.

- **R² = 0.70**:  
  70% of the variance is explained by the model; 30% is due to unknown or inherent variability.

---

## 🧮 How R-squared Is Calculated

R-squared compares the **Sum of Squared Errors (SSE)** to the **Total Sum of Squares (SST)**:



$$
R^2 = 1 - \frac{\text{SSE}}{\text{SST}}
$$



- **SSE (Sum of Squared Errors)**:  
  The sum of squared differences between predicted and actual values. Represents unexplained variance.

- **SST (Total Sum of Squares)**:  
  The sum of squared differences between actual values and their mean. Represents total variance.

---

## ⚠️ Limitations of R-squared

While useful, R-squared has several limitations:

- **Favors complex models**:  
  Adding more variables always increases R², even if they don't improve predictive power → risk of overfitting.

- **Does not indicate model quality**:  
  A high R² isn't always good, and a low R² isn't always bad—especially in fields like psychology or behavior modeling.

- **Does not detect bias**:  
  A biased model can still have a high R². Residual plots are needed to assess bias.

- **Does not imply causation**:  
  High R² shows correlation, not causation.

---

## 🔄 R-squared vs. Adjusted R-squared

| Metric              | Description                                                                 |
|---------------------|-----------------------------------------------------------------------------|
| **R-squared**        | Increases with more predictors, regardless of their relevance.              |
| **Adjusted R-squared** | Penalizes irrelevant predictors. Only increases if a new variable improves the model beyond chance. |

Many data scientists prefer **adjusted R-squared** when comparing models with different numbers of predictors.

---

## 🐍 Calculating R-squared in Python

You can compute R² using `r2_score` from `sklearn.metrics`:


``` python

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score

# Assume you have your actual (true) values and the values predicted by your AI model
y_true = np.array([10, 20, 30, 40, 50])  # Example: actual observed data points
y_pred = np.array([11, 19, 32, 38, 51])  # Example: values predicted by your AI model

# Calculate the R-squared score
r2 = r2_score(y_true, y_pred)
print(f"R-squared (R²) score: {r2:.4f}")

# Create the plot
fig, ax = plt.subplots(figsize=(8, 6))

# Scatter plot of actual vs. predicted values
ax.scatter(y_true, y_pred, color='blue', alpha=0.7, label='Predicted vs. True')

# Add a diagonal line for perfect predictions
ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()],
        '--', color='red', lw=2, label='Perfect Prediction')

# Add the R-squared value as an annotation
ax.annotate(f"R² = {r2:.4f}", xy=(0.05, 0.9), xycoords='axes fraction', fontsize=12,
            bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="black", lw=1))

# Set labels and title
ax.set_title('Predicted vs. Actual Values')
ax.set_xlabel('True Values')
ax.set_ylabel('Predicted Values')

# Add a legend
ax.legend()

# Add a grid for better readability
ax.grid(True)

# Display the plot
plt.tight_layout()
plt.show()



```
