# 📊 R-squared Score

The **R-squared (R²) score**, also known as the **coefficient of determination**, is a statistical measure that indicates how well the independent variable(s) in a regression model explain the variation in the dependent variable. It represents the proportion of the dependent variable's variance that is predictable from the independent variable(s).

---

## 🎯 Interpretation of R-squared

R-squared values range from 0 to 1:

- **R² = 0**:  
  The model explains none of the variability. It's no better than using the mean of the dependent variable.

- **R² = 1**:  
  The model explains all variability. Predictions perfectly match the observed data.

- **R² = 0.70**:  
  70% of the variance is explained by the model; 30% is due to unknown or inherent variability.

---

## 🧮 How R-squared Is Calculated

R-squared compares the **Sum of Squared Errors (SSE)** to the **Total Sum of Squares (SST)**:



$$
R^2 = 1 - \frac{\text{SSE}}{\text{SST}}
$$



- **SSE (Sum of Squared Errors)**:  
  The sum of squared differences between predicted and actual values. Represents unexplained variance.

- **SST (Total Sum of Squares)**:  
  The sum of squared differences between actual values and their mean. Represents total variance.

---

## ⚠️ Limitations of R-squared

While useful, R-squared has several limitations:

- **Favors complex models**:  
  Adding more variables always increases R², even if they don't improve predictive power → risk of overfitting.

- **Does not indicate model quality**:  
  A high R² isn't always good, and a low R² isn't always bad—especially in fields like psychology or behavior modeling.

- **Does not detect bias**:  
  A biased model can still have a high R². Residual plots are needed to assess bias.

- **Does not imply causation**:  
  High R² shows correlation, not causation.

---

## 🔄 R-squared vs. Adjusted R-squared

| Metric              | Description                                                                 |
|---------------------|-----------------------------------------------------------------------------|
| **R-squared**        | Increases with more predictors, regardless of their relevance.              |
| **Adjusted R-squared** | Penalizes irrelevant predictors. Only increases if a new variable improves the model beyond chance. |

Many data scientists prefer **adjusted R-squared** when comparing models with different numbers of predictors.

---

## 🐍 Calculating R-squared in Python

You can compute R² using `r2_score` from `sklearn.metrics`:


``` python

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score

# Assume you have your actual (true) values and the values predicted by your AI model
y_true = np.array([10, 20, 30, 40, 50])  # Example: actual observed data points
y_pred = np.array([11, 19, 32, 38, 51])  # Example: values predicted by your AI model

# Calculate the R-squared score
r2 = r2_score(y_true, y_pred)
print(f"R-squared (R²) score: {r2:.4f}")

# Create the plot
fig, ax = plt.subplots(figsize=(8, 6))

# Scatter plot of actual vs. predicted values
ax.scatter(y_true, y_pred, color='blue', alpha=0.7, label='Predicted vs. True')

# Add a diagonal line for perfect predictions
ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()],
        '--', color='red', lw=2, label='Perfect Prediction')

# Add the R-squared value as an annotation
ax.annotate(f"R² = {r2:.4f}", xy=(0.05, 0.9), xycoords='axes fraction', fontsize=12,
            bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="black", lw=1))

# Set labels and title
ax.set_title('Predicted vs. Actual Values')
ax.set_xlabel('True Values')
ax.set_ylabel('Predicted Values')

# Add a legend
ax.legend()

# Add a grid for better readability
ax.grid(True)

# Display the plot
plt.tight_layout()
plt.show()



```
# 📈 R-squared (R²) Score in Machine Learning

The **R-squared (R²) score**, also known as the **coefficient of determination**, is a statistical metric that evaluates how well the independent variables in a regression model explain the variation in the dependent variable.

---

## 🧪 Python Code Explanation

### 🔍 Line-by-Line Breakdown
* ```import numpy as np```: Imports NumPy for numerical operations and array handling.

* ```from sklearn.metrics import r2_score```: Imports the ```r2_score``` function from scikit-learn.

* ```y_true```: Actual observed values.

* ```y_pred```: Predicted values from the model.

* ```r2_score(...)```: Calculates the R² score.

* ```print(...)```: Displays the R² score.

This snippet provides a simple way to evaluate how well your model's predictions align with actual data.


### 📊 Graph Interpretation
* **Scatter Points**: Blue dots represent actual vs. predicted values.

* **Perfect Prediction Line**: Red dashed line where ```y_pred = y_true```.

* **Annotation**: R² value shown on the plot for quick reference.

### ❓ What Does R-squared Measure?
R² quantifies the proportion of variance in the dependent variable explained by the independent variables. It ranges from 0 to 1, often expressed as a percentage:

* **0%**: Model explains none of the variability.

* **100%**: Model perfectly explains all variability.

* **70%**: Model explains 70% of the variance; 30% remains unexplained.

### Coefficient of Determination (R²)

$$
R^2 = \frac{\text{Explained Variation}}{\text{Total Variation}} 
= 1 - \frac{\text{SSE}}{\text{SST}}
$$

Where:  
- **SSE** = Sum of Squared Errors (unexplained variation)  
- **SST** = Total Sum of Squares (total variation)  


### ⚠️ Limitations of R-squared

* **Overfitting**: R² increases with more predictors, even irrelevant ones.
* **No Causation**: High R² ≠ causal relationship.
* **Bias Detection**: R² doesn't reveal prediction bias—use residual plots.
* **Goodness of Fit**: High R² doesn't guarantee a good model.
* **Non-linear Models**: R² may misrepresent performance for non-linear relationships.

### 🔄 Alternatives and Considerations

| Metric            | Description                                                     |
|-------------------|-----------------------------------------------------------------|
| Adjusted R-squared| Penalizes unnecessary predictors; better for comparing models.  |
| MSE / RMSE        | Quantifies average error magnitude.                             |
| MAE               | Measures average absolute error.                                |

Always combine R² with residual analysis and domain knowledge for robust evaluation.

# 🎥 Recommended Videos

## 🧠 Beginner-Friendly

[R-squared, Clearly Explained!!! – StatQuest](https://www.youtube.com/watch?v=2AQKmw14mHM)              
[R Squared Explained in 5 Minutes – Enjoy Machine Learning](https://www.youtube.com/watch?v=ZkjP5RJLQF4)



# 🧠 Key Concepts Recap

* **What it measures**: Proportion of variance explained by the model.
* **Score range**: 0 to 1 (or 0% to 100%).
* **Interpretation**: Higher R² = better fit, but beware of overfitting.
* **Adjusted R²**: More reliable for models with multiple predictors.

## [Context](./../context.md)