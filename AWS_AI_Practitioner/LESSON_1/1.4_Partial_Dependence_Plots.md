# 1.4 Partial Dependence Plots

# ðŸŽ¯ What Are Partial Dependence Plots?
PDPs show how a feature (or a pair of features) influences the predicted outcome of a machine learning model, **averaging out the effects of all other features**. They help answer questions like: *"How does changing feature X affect the prediction, on average?"*

# ðŸ“Š How PDPs Work

* You fix the value of one feature (say, age = 30).
* Then, you vary the other features across the dataset and compute the modelâ€™s predictions.
* Finally, you average those predictions to see the effect of age = 30 on the outcome.

This gives you a curve or surface that visualizes the average model response to changes in that feature.

# ðŸ“º YouTube Resources
Hereâ€™s a great video to get started:

[Partial Dependence Plots (PDPs) maths explained](https://www.youtube.com/watch?v=FHooiNyREvg) â€“ This video walks through the math behind PDPs and how theyâ€™re used in explainable AI (XAI)

[Partial Dependence Plots (Opening the Black Box) - ritvikmath - youtube](https://www.youtube.com/watch?v=uQQa3wQgG_s)

ðŸ§  Why PDPs Matter

* Useful for **model transparency**, especially in black-box models like random forests or gradient boosting.
* Helps identify **non-linear relationships** and **feature interactions**.
* Can guide feature **engineering** and model **debugging**.


 ## [Context](./../context.md)
