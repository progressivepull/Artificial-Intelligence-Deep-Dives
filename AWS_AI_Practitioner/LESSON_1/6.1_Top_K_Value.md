# 6.1 Top K Value

# Understanding Top K in Generative AI

## Overview

**Top K** is a hyperparameter used in generative AI, especially in Large Language Models (LLMs), to control the diversity and creativity of generated text. It limits the pool of potential tokens (words or subwords) to the **K most probable options** for each step in the generation process.

For every token it generates, a language model assigns a probability to every word in its vocabulary. For example, given the prompt `"The cat sat on the..."`, the model might assign high probabilities to `"mat"`, `"chair"`, or `"rug"`, and low probabilities to `"computer"` or `"banana"`. Top K filters this list to include only the highest-probability candidates.

## How Top K Affects AI Output

Adjusting the integer value of K directly influences the balance between predictability and creativity.

### Low Top K (e.g., K = 1 to 5)

- **Effect**: The model becomes highly deterministic, selecting from a very small pool of top tokens.
- **Output**: Predictable, conventional, and coherent—but potentially repetitive or lacking creativity.
- **Best For**:  
  - Summarization  
  - Factual question-answering  
  - Technical documentation

### High Top K (e.g., K = 50 or higher)

- **Effect**: The model considers a broader range of tokens, including those with lower probabilities.
- **Output**: More diverse and creative, with potential for surprising or imaginative results. However, very high values may lead to irrelevant or incoherent output.
- **Best For**:  
  - Brainstorming  
  - Poetry or fiction writing  
  - Dialogue generation for chatbots

## Top K vs. Other Generation Parameters

| Parameter       | Control Mechanism                                           | Effect on Output                                                                 |
|----------------|-------------------------------------------------------------|----------------------------------------------------------------------------------|
| **Top K**       | Limits choices to the K most probable tokens               | Balances coherence and creativity by restricting to a fixed number of candidates |
| **Temperature** | Scales the probability distribution of all tokens          | Adjusts randomness; higher values increase creativity and risk of incoherence    |
| **Top P**       | Includes tokens until cumulative probability exceeds `p`   | Offers dynamic flexibility based on distribution shape, not fixed token count    |

## Summary

Top K is a powerful tool for tuning the behavior of generative models. By adjusting this parameter, developers can control how predictable or inventive the model’s output will be. It is often used in combination with **temperature** and **Top P (nucleus sampling)** to strike the right balance for a given task.

* [What are the LLM’s Top-P + Top-K ?](https://www.youtube.com/watch?v=aDmp2Uim0zQ)
* [LLM Basics: Top-p vs. Top-K Sampling Explained for Beginners](https://www.youtube.com/watch?v=_3DWwb96exY)

## [Context](./../context.md)

