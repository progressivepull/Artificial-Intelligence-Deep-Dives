# Accuracy in Machine Learning

**Accuracy** is a metric that measures how often a classification model's predictions are correct overall. While it is a simple and intuitive metric, it is not a complete measure of a model's overall performance.

## What Accuracy Measures

In machine learning, accuracy is defined as the ratio of correct predictions to the total number of predictions made by the model:

$$
\text{Accuracy} = \frac{\text{Correct Predictions}}{\text{Total Predictions}}
$$

For a binary classification model, this can also be expressed using a **confusion matrix**, which visualizes the model's true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN):

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

## Limitations of Using Only Accuracy

Accuracy can be a misleading metric, especially when dealing with **imbalanced datasets** where one class is far more common than another. In these cases, a model can achieve a high accuracy score by simply predicting the majority class for every instance.

### Example:
A model designed to detect fraudulent transactions might achieve 99.9% accuracy by always predicting "not fraudulent," because fraudulent transactions are so rare. This high accuracy is deceptive, as the model completely fails at its intended purpose.

## Other Performance Metrics for Classification

Because of its limitations, **accuracy** is often considered alongside other metrics to provide a more comprehensive view of a model's performance:

### 1. **Precision**
Measures how many of the model's positive predictions were actually correct. It is important when the cost of a false positive is high.

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

### 2. **Recall (Sensitivity)**
Measures how many of the actual positive cases the model correctly identified. It is important when the cost of a false negative is high.

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

### 3. **F1 Score**
The harmonic mean of precision and recall. It provides a single score that balances both metrics and is useful for imbalanced datasets.

$$
\text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

### 4. **Area Under the Curve (AUC-ROC)**
Evaluates how well a model can distinguish between classes. A higher AUC score indicates better performance across all classification thresholds.

### 5. **Confusion Matrix**
A table that provides a detailed breakdown of a model's correct and incorrect predictions for each class.

## Performance Metrics for Regression

For regression models, which predict a continuous value rather than a class, accuracy is not used. Instead, performance is measured by how close the predictions are to the actual values. Common metrics include:

### 1. **Mean Absolute Error (MAE)**
The average of the absolute differences between the predicted and actual values.

### 2. **Mean Squared Error (MSE)**
The average of the squared differences between the predicted and actual values. It penalizes larger errors more heavily.

### 3. **Root Mean Squared Error (RMSE)**
The square root of the MSE, which is in the same units as the predicted values.


[10 Tips for Improving the Accuracy of your Machine Learning Models](https://www.youtube.com/watch?v=uqhn5xuyQok)                                                             
[46. Evaluation of model performance accuracy and area under the curve AUC](https://www.youtube.com/watch?v=2aPmYvlaZvU)

## [Context](./../context.md)
