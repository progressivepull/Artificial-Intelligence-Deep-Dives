# 1.2 Evaluation Metrics for Classification 

# ðŸ§  Core Evaluation Metrics for Binary Classification

## ðŸ“Š Classification Metric Summary

| **Metric**   | **Formula**                                                                 | **What It Measures**                                      | **When It's Important**                                                                 |
|--------------|------------------------------------------------------------------------------|------------------------------------------------------------|------------------------------------------------------------------------------------------|
| Accuracy     | $$\frac{TP + TN}{TP + TN + FP + FN}$$                                       | Overall correctness of predictions                         | Useful when classes are balanced and all errors are equally costly                      |
| Precision    | $$\frac{TP}{TP + FP}$$                                                      | How many predicted positives are actually correct          | Crucial when false positives are costly (e.g., spam detection, medical diagnosis)       |
| Recall       | $$\frac{TP}{TP + FN}$$                                                      | How many actual positives were correctly identified        | Vital when missing a positive is costly (e.g., fraud detection, cancer screening)       |
| F1 Score     | $$\frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$$ | Balance between precision and recall                       | Best when you need a trade-off between precision and recall                             |

 
 
 ## [Context](./../context.md)