# 1.2 Evaluation Metrics for Classification 

# üß† Core Evaluation Metrics for Binary Classification

## üìä Classification Metric Summary

![Confusion_Matrices.png](./IMAGES/Confusion_Matrices.png)

Using a medical test for a disease as an example:
* **True Positive (TP)**: The model correctly predicts a positive outcome, such as correctly identifying a patient with the disease.

* **True Negative (TN)**: The model correctly predicts a negative outcome, such as correctly identifying a healthy patient.

* **False Positive (FP)**: The model incorrectly predicts a positive outcome (Type I error or "false alarm"), such as predicting a patient has the disease when they are healthy.

* **False Negative (FN)**: The model incorrectly predicts a negative outcome (Type II error or "missed detection"), such as predicting a patient is healthy when they actually have the disease.¬†

| **Metric**   | **Formula**                                                                 | **What It Measures**                                      | **When It's Important**                                                                 |
|--------------|------------------------------------------------------------------------------|------------------------------------------------------------|------------------------------------------------------------------------------------------|
| Accuracy     | $$\frac{TP + TN}{TP + TN + FP + FN}$$                                       | Overall correctness of predictions                         | Useful when classes are balanced and all errors are equally costly                      |
| Precision    | $$\frac{TP}{TP + FP}$$                                                      | How many predicted positives are actually correct          | Crucial when false positives are costly (e.g., spam detection, medical diagnosis)       |
| Recall       | $$\frac{TP}{TP + FN}$$                                                      | How many actual positives were correctly identified        | Vital when missing a positive is costly (e.g., fraud detection, cancer screening)       |
| F1 Score     | $$\frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$$ | Balance between precision and recall                       | Best when you need a trade-off between precision and recall                             |


## üéØ Why Metric Choice Depends on the Problem

### üìß Spam Detection
- **Precision** is key ‚Äî you don‚Äôt want to wrongly classify important emails as spam  
- **Recall** also matters ‚Äî you want to catch as much spam as possible  

### üè• Medical Diagnosis
- **Recall** is often prioritized ‚Äî you don‚Äôt want to miss a disease  
- **Precision** is also important ‚Äî you don‚Äôt want to falsely alarm healthy patients  

### üí≥ Credit Card Fraud Detection
- **Recall** is critical ‚Äî missing fraud can be costly  
- **F1 Score** helps balance the trade-off between catching fraud and avoiding false alarms  


## üçé Metrics for Multi-Class Classification

| **Metric**                 | **Purpose**                                                    | **Use Case**                                                             |
|----------------------------|----------------------------------------------------------------|--------------------------------------------------------------------------|
| Categorical Cross-Entropy  | Measures how far predicted probabilities are from actual class labels | Used in probabilistic models like neural networks                         |
| Macro F1 Score             | Averages F1 scores across all classes equally                  | Good when all classes are equally important                              |
| Micro F1 Score             | Aggregates contributions of all classes to compute a global F1 | Useful when class imbalance exists                                       |
| Top-k Accuracy             | Checks if the correct label is among the top-k predictions     | Helpful in recommendation systems or image classification tasks          |


 ## üß≠ Summary Takeaways

- **No single metric tells the whole story** ‚Äî context matters  
- **Binary vs. multi-class problems** require different evaluation strategies  
- **Align your metric choice** with the real-world consequences of prediction errors  

 
 ## [Context](./../context.md)