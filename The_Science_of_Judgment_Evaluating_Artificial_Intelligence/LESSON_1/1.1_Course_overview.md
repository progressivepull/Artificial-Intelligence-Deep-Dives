# 1.1 Course overview 

## Course Title
The Science of Judgment Evaluating Artificial Intelligence

## Presented By
Dr. Elian Voss

## About the Presented
In the year 2419, Earth‚Äôs historians are no longer scholars‚Äîthey‚Äôre timewalkers. Using quantum resonance chambers, they travel into the past to preserve endangered truths before they‚Äôre erased by the ‚ÄúChrono Drift,‚Äù a mysterious phenomenon that rewrites history itself.

Dr. Elian Voss, a renowned timewalker and archivist, uncovers a hidden anomaly in 16th-century Florence: a secret society of polymaths who discovered a primitive form of quantum entanglement centuries before it was theorized. But when she tries to extract their knowledge, she triggers a paradox that begins unraveling the timeline from both ends.


## üéØ Course Objectives

This course explores how to measure and evaluate the performance of machine learning models. You'll learn:

- **Key metrics** for classification and regression tasks  
- **Common challenges** in model evaluation  
- **Techniques** to mitigate evaluation pitfalls  
- **Best practices** for robust model assessment  
- **Emerging trends** in AI/ML evaluation  
----

## üß™ Evaluation Metrics

### üîπ Classification Metrics  
Used when predicting discrete labels (e.g., spam vs. not spam):

| **Metric**         | **Description**                                 | **Use Case**                          |
|--------------------|--------------------------------------------------|----------------------------------------|
| Accuracy           | % of correct predictions                         | Balanced datasets                      |
| Precision          | % of predicted positives that are correct        | When false positives are costly        |
| Recall (Sensitivity)| % of actual positives correctly identified      | When false negatives are costly        |
| F1 Score           | Harmonic mean of precision and recall            | Imbalanced datasets                    |
| ROC-AUC            | Area under the ROC curve                         | Binary classification                  |
| Confusion Matrix   | Breakdown of TP, FP, TN, FN                      | Diagnostic tool                        |

### üîπ Regression Metrics  
Used when predicting continuous values (e.g., house prices):

| **Metric**                     | **Description**                  | **Use Case**                |
|-------------------------------|----------------------------------|-----------------------------|
| Mean Absolute Error (MAE)     | Average of absolute errors       | Easy to interpret           |
| Mean Squared Error (MSE)      | Average of squared errors        | Penalizes large errors      |
| Root Mean Squared Error (RMSE)| Square root of MSE               | Same units as target        |
| R¬≤ Score (Coefficient of Determination) | % of variance explained | Overall model fit           |

----

## ‚ö†Ô∏è Evaluation Challenges

### 1. Overfitting
- Model performs well on training data but poorly on unseen data  
- Caused by excessive complexity or lack of regularization  

### 2. Bias
- Systematic error due to flawed assumptions or data  
- Can lead to unfair or inaccurate predictions  

### 3. Class Imbalance
- One class dominates the dataset (e.g., 95% negative, 5% positive)  
- Accuracy becomes misleading; minority class gets ignored  

----

## üõ†Ô∏è Mitigation Techniques

| **Challenge**      | **Mitigation Strategies**                                      |
|--------------------|---------------------------------------------------------------|
| Overfitting        | Cross-validation, regularization, pruning                     |
| Bias               | Fairness-aware metrics, diverse training data                 |
| Class Imbalance    | Resampling (SMOTE, undersampling), weighted loss functions    |

----

## ‚úÖ Best Practices

- Use **cross-validation** to assess generalization  
- Always **visualize metrics** (e.g., ROC curves, residual plots)  
- Choose **metrics aligned with business goals**  
- Monitor **drift over time** in production models  
- Document **assumptions and limitations** clearly  

---

## üöÄ Emerging Trends

- **Explainable AI (XAI):** Metrics that assess interpretability  
- **Fairness Metrics:** Equal opportunity, demographic parity  
- **Robustness Testing:** Evaluating models under adversarial conditions  
- **Data-centric Evaluation:** Emphasis on dataset quality and coverage  
- **Human-in-the-loop Evaluation:** Combining automated metrics with expert review  


 
 ## [Context](./../context.md)