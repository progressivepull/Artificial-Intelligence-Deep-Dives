# 1.8 False Positives and False Negatives in Decision making 

Let‚Äôs take this deeper and turn the video‚Äôs insights into a structured, intuitive guide for analyzing false positives vs. false negatives in decision-making‚Äîespecially how they shape model evaluation and real-world impact.
 
 # ‚öñÔ∏è False Positives vs. False Negatives: Core Definitions

 ## üìä Classification Errors

| Term                  | Description                                                                                  | Error Type   |
|-----------------------|----------------------------------------------------------------------------------------------|--------------|
| **False Positive (FP)** | Model incorrectly predicts a **positive** outcome for a negative case (e.g., flags a legitimate transaction as fraud) | **Type I Error** |
| **False Negative (FN)** | Model incorrectly predicts a **negative** outcome for a positive case (e.g., misses a fraudulent transaction)         | **Type II Error** |

These errors are inevitable in classification models. The key is strategic prioritization based on context.

 # ‚öñÔ∏è Error Trade-offs in Classification Models  

These errors are **inevitable** in classification models.  
The key is **strategic prioritization based on context**.  

---

## üß™ Trade-off Analysis by Use Case  

### 1. Criminal Identification System  
- **False Positive**: Innocent person labeled as criminal  
- **False Negative**: Criminal labeled as innocent  
- **Priority**: Minimize **false positives** to avoid wrongful accusations  
- **Metric to Focus On**: **Precision**
  
$$Precision = \frac{TP}{TP + FP}$$ 

High precision ensures that when the model flags someone as a criminal, it‚Äôs likely correct.  

---

### 2. Medical Diagnosis System  
- **False Positive**: Healthy person diagnosed as sick  
- **False Negative**: Sick person diagnosed as healthy  
- **Priority**: Minimize **false negatives** to avoid missed treatments  
- **Metric to Focus On**: **Recall**  

$$Recall = \frac{TP}{TP + FN}$$

High recall ensures that most sick individuals are correctly identified and treated.  

---

### 3. Promotion Eligibility Model  
- **False Positive**: Undeserving employee promoted  
- **False Negative**: Deserving employee overlooked  
- **Priority**: Minimize **both errors** to maintain fairness and morale  
- **Metric to Focus On**: **F1 Score**  

$$F1 = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}$$
 
The F1 score balances precision and recall, making it ideal when **both error types carry significant cost**.  

---

## üéØ Threshold Tuning: The Balancing Act  
Classification models often use a **probability threshold** (e.g., 0.5) to decide class labels.  
Adjusting this threshold shifts the balance between **FP** and **FN**:  

- Lower threshold ‚Üí More positives ‚Üí ‚Üì FN, ‚Üë FP  
- Higher threshold ‚Üí Fewer positives ‚Üí ‚Üì FP, ‚Üë FN  

Use **ROC curves** and **Precision-Recall curves** to visualize and select optimal thresholds based on your use case.  

---

## üß≠ Real-World Decision Framework  
When deciding **which error to minimize**:  
1. Assess the **cost of each error** (financial, ethical, reputational)  
2. Engage **stakeholders** to understand priorities  
3. Choose **metrics aligned with goals**  
4. Continuously **monitor performance** with real-world feedback  

 ## [Context](./../context.md)
