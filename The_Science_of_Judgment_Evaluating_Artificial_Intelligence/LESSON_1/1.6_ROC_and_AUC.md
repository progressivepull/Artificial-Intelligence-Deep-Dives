# 1.6 ROC and AUC 

# 📊 ROC Curve: Visualizing Trade-offs
The **Receiver Operating Characteristic (ROC)** curve is a graphical tool that shows how well a binary classifier distinguishes between two classes across different threshold values.
 
 # 🔍 Axes of the ROC Curve  

- **X-axis:** False Positive Rate (FPR)

$$FPR = \frac{FP}{FP + TN}$$

- **Y-axis:** True Positive Rate (TPR) (also called *Recall*)

$$TPR = \frac{TP}{TP + FN}$$

Each point on the ROC curve represents a **TPR/FPR pair** for a specific threshold.  
As the threshold changes, the balance between **sensitivity** and **specificity** shifts.

# 🧠 Intuition
* A model that **perfectly separates** classes will have a curve that hugs the **top-left corner**.

* A model that performs **no better than random guessing** will produce a diagonal line from (0,0) to (1,1).

# 📐 AUC: Quantifying Performance
**Area Under the Curve (AUC)** is a scalar value summarizing the ROC curve. It represents the probability that the classifier ranks a randomly chosen positive instance higher than a randomly chosen negative one.

# 📊 AUC Value Interpretation

| AUC Value | Interpretation                     |
|-----------|------------------------------------|
| **1.0**   | Perfect classifier                 |
| **0.9 – 1.0** | Excellent                      |
| **0.8 – 0.9** | Good                           |
| **0.7 – 0.8** | Fair                           |
| **0.6 – 0.7** | Poor                           |
| **0.5**   | No discrimination (random guess)   |
| **< 0.5** | Worse than random                  |


# 🧪 Example: Customer Review Classification

Suppose we’re classifying **110 reviews**:  
- **77 positive**  
- **33 negative**  

The model outputs probabilities for each review being positive.  
We vary the threshold (e.g., **0.3, 0.5, 0.7**) and compute **TPR** and **FPR** at each.

---

## 🔄 Threshold Tuning
- At **0.5**, the model might misclassify some negative reviews as positive.  
- Lowering the threshold increases **TPR** but may also increase **FPR**.  
- The **ROC curve** helps visualize this trade-off.  
- The **AUC** helps compare models: higher AUC = better overall discrimination.  

---

## 🧭 When to Use ROC/AUC
ROC and AUC are especially useful when:  
- You want to evaluate performance **across all thresholds**  
- You care about **ranking quality**, not just classification accuracy  
- Your classes are **balanced** or you want to assess **ranking robustness**  

# 🛠️ Practical Tip
In Python (e.g., using scikit-learn), you can compute ROC and AUC like this:

``` python
from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(y_true, y_scores)
roc_auc = auc(fpr, tpr)

```
You can then plot the ROC curve to visualize performance.


 
 ## [Context](./../context.md)
